# robots.txt for UniLink
# This file tells search engines how to crawl your site

# Allow all search engines to crawl everything
User-agent: *
Allow: /

# Disallow crawling of API endpoints or private pages (if any)
# Disallow: /api/
# Disallow: /admin/

# Sitemap location (update URL after deployment)
Sitemap: https://unilink.ng/sitemap.xml

# Crawl delay (optional - prevents aggressive crawling)
# Crawl-delay: 1

# Specific rules for common bots
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /
